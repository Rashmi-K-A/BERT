{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Q_A_simple",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N6JmjG24C-Z",
        "outputId": "b531bc95-8306-4540-eec3-dfced9c4096e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLfHIKl-4Vik"
      },
      "source": [
        "### Load question answering and transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN-IODVj4TPs"
      },
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#Model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf2Qsb8Z4kxI"
      },
      "source": [
        "Note: The BertForQuestionAnswering class supports fine-tunning. We can fine-tune this model on our own dataset.\n",
        "\n",
        "Create a QA example and use function encode_plus() to encode the example. The function encode_plus() returns a dictionary that contains input_ids, token_type_ids, and attention mask but we only need input_ids and token_type_ids for the QA task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDrvnr_J4l7N"
      },
      "source": [
        "question = '''What is minimum spanning tree?'''\n",
        "\n",
        "paragraph = ''' In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. \n",
        "                This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges \n",
        "                in the tree is minimized.'''\n",
        "            \n",
        "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special_tokens=True)\n",
        "\n",
        "inputs = encoding['input_ids']  #Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8cpci7j5gRE"
      },
      "source": [
        "Note: In the case of multiple QA examples, we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0.\n",
        "\n",
        "Run the QA example through the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2vHKeGG5leo"
      },
      "source": [
        "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrUXw_t5z1i"
      },
      "source": [
        "\n",
        "\n",
        "Now we have start scores and end scores we can get both the start index and the end index and use both the indices for span prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLvn1GOI52qb",
        "outputId": "4cddb185-488a-4d21-8d33-295de31affd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "start_index = torch.argmax(start_scores)\n",
        "\n",
        "end_index = torch.argmax(end_scores)\n",
        "\n",
        "answer = ' '.join(tokens[start_index:end_index+1])\n",
        "answer"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a subset of the edges that forms a tree that includes every vertex'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AESqGksv5_fz"
      },
      "source": [
        "\n",
        "\n",
        "Note: The model is likely to predict an end word that is before the start word. The correct way is to pick a span for which the total score (start score + end score) is maximum where end_index ≥ start_index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGYGD5fq6ZhS"
      },
      "source": [
        "Note: BERT uses wordpiece tokenization. Wordpiece split the tokens like “playing” to “play and ##ing”. It also covers a wider spectrum of Out-Of-Vocabulary (OOV) words.\n",
        "\n",
        "We can recover any words that were broken down into subwords with a little bit more work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GGNsPC46e9N"
      },
      "source": [
        "corrected_answer = ''\n",
        "\n",
        "for word in answer.split():\n",
        "    \n",
        "    #If it's a subword token\n",
        "    if word[0:2] == '##':\n",
        "        corrected_answer += word[2:]\n",
        "    else:\n",
        "        corrected_answer += ' ' + word"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbLBBuLa6f5r",
        "outputId": "7550a448-1550-452a-e872-77513ea99d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(question)\n",
        "print(corrected_answer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is minimum spanning tree?\n",
            " a subset of the edges that forms a tree that includes every vertex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1HGzOxs6mSP"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}