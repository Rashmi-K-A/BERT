{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Q_A_checking",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3cYhI3mtypBFT+1kfikGn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalpanab-psg/BERT/blob/main/BERT_Q_A_checking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2XBoIiabPAD"
      },
      "source": [
        "# BERT Question - Answering\n",
        "Lets understand how we can apply a fine-tuned BERT to question answering tasks i.e given a question and a passage containing the answer, the task is to predict the answer text span in the paragraph given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLfHIKl-4Vik"
      },
      "source": [
        "### Load question answering and transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fCqXxoEbJlX",
        "outputId": "7f0345e4-4b4f-4b23-d275-596cfe5be751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN-IODVj4TPs"
      },
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#Model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf2Qsb8Z4kxI"
      },
      "source": [
        "Note: The BertForQuestionAnswering class supports fine-tunning. We can fine-tune this model on our own dataset.\n",
        "\n",
        "Create a QA example and use function encode_plus() to encode the example. The function encode_plus() returns a dictionary that contains input_ids, token_type_ids, and attention mask but we only need input_ids and token_type_ids for the QA task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuALskK8Womu"
      },
      "source": [
        "paragraph = ''' Prim's (also known as Jarník's) algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. \n",
        "                This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in \n",
        "                the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each\n",
        "                 step adding the cheapest possible connection from the tree to another vertex.'''\n",
        "\n",
        "\n",
        "question = '''prims algorithm works on what type of graph?'''\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDrvnr_J4l7N",
        "outputId": "2ceab88d-0b7e-4c21-c182-294bd0104903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special_tokens=True)\n",
        "\n",
        "inputs = encoding['input_ids']  #Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
        "\n",
        "print(\"\\nToken embeddings\")\n",
        "print(inputs)\n",
        "print(\"\\nSentence Embedding\")\n",
        "print(sentence_embedding)\n",
        "print(\"\\nTokens\")\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Token embeddings\n",
            "[101, 26927, 5244, 9896, 2573, 2006, 2054, 2828, 1997, 10629, 1029, 102, 26927, 2213, 1005, 1055, 1006, 2036, 2124, 2004, 15723, 8238, 1005, 1055, 1007, 9896, 2003, 1037, 20505, 9896, 2008, 4858, 1037, 6263, 13912, 3392, 2005, 1037, 18215, 6151, 7442, 10985, 10629, 1012, 2023, 2965, 2009, 4858, 1037, 16745, 1997, 1996, 7926, 2008, 3596, 1037, 3392, 2008, 2950, 2296, 19449, 1010, 2073, 1996, 2561, 3635, 1997, 2035, 1996, 7926, 1999, 1996, 3392, 2003, 18478, 2094, 1012, 1996, 9896, 5748, 2011, 2311, 2023, 3392, 2028, 19449, 2012, 1037, 2051, 1010, 2013, 2019, 15275, 3225, 19449, 1010, 2012, 2169, 3357, 5815, 1996, 10036, 4355, 2825, 4434, 2013, 1996, 3392, 2000, 2178, 19449, 1012, 102]\n",
            "\n",
            "Sentence Embedding\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Tokens\n",
            "['[CLS]', 'pri', '##ms', 'algorithm', 'works', 'on', 'what', 'type', 'of', 'graph', '?', '[SEP]', 'pri', '##m', \"'\", 's', '(', 'also', 'known', 'as', 'jar', '##nik', \"'\", 's', ')', 'algorithm', 'is', 'a', 'greedy', 'algorithm', 'that', 'finds', 'a', 'minimum', 'spanning', 'tree', 'for', 'a', 'weighted', 'und', '##ire', '##cted', 'graph', '.', 'this', 'means', 'it', 'finds', 'a', 'subset', 'of', 'the', 'edges', 'that', 'forms', 'a', 'tree', 'that', 'includes', 'every', 'vertex', ',', 'where', 'the', 'total', 'weight', 'of', 'all', 'the', 'edges', 'in', 'the', 'tree', 'is', 'minimize', '##d', '.', 'the', 'algorithm', 'operates', 'by', 'building', 'this', 'tree', 'one', 'vertex', 'at', 'a', 'time', ',', 'from', 'an', 'arbitrary', 'starting', 'vertex', ',', 'at', 'each', 'step', 'adding', 'the', 'cheap', '##est', 'possible', 'connection', 'from', 'the', 'tree', 'to', 'another', 'vertex', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8cpci7j5gRE"
      },
      "source": [
        "Note: In the case of multiple QA examples, we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0.\n",
        "\n",
        "Run the QA example through the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2vHKeGG5leo",
        "outputId": "5ec32b95-aea7-4dd3-d1b9-f371cc4972d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
        "print(\"start_scores\\n \",start_scores)\n",
        "print(\"end scores\\n\", end_scores)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start_scores\n",
            "  tensor([[-6.3971, -8.3209, -9.1853, -8.6808, -8.2087, -7.9485, -6.6165, -8.8133,\n",
            "         -8.8498, -9.0359, -9.9972, -6.3971, -3.8731, -7.7654, -8.4206, -7.8233,\n",
            "         -7.7760, -7.5092, -8.4712, -8.7257, -6.4428, -8.4916, -8.6429, -8.1336,\n",
            "         -7.7421, -6.0585, -6.7516, -5.2587, -3.3205, -5.9829, -6.9598, -4.8603,\n",
            "         -5.6328, -4.3468, -5.8368, -5.5401, -3.1907,  5.6977,  7.3329,  6.1563,\n",
            "         -2.7407, -2.2136, -1.5076, -5.6443, -6.2299, -7.4219, -6.7675, -6.2468,\n",
            "         -6.7222, -6.4435, -8.4497, -8.0244, -6.3336, -8.6385, -7.5787, -6.2134,\n",
            "         -4.9520, -8.5590, -7.7317, -7.3434, -5.6952, -7.9661, -7.1608, -6.6087,\n",
            "         -6.4694, -4.7526, -8.7338, -8.2227, -8.8020, -6.9529, -8.8522, -7.7709,\n",
            "         -5.6715, -8.5009, -5.7772, -7.7641, -7.5371, -6.3379, -6.8356, -7.2648,\n",
            "         -6.6319, -5.7156, -7.3248, -5.7208, -7.3504, -7.0822, -8.7211, -8.7740,\n",
            "         -7.9832, -8.0928, -7.2924, -7.9072, -6.0518, -7.1756, -6.8299, -8.3008,\n",
            "         -7.1942, -7.7740, -7.8549, -6.5448, -7.8233, -7.2088, -8.5768, -8.4539,\n",
            "         -7.0040, -8.8120, -7.9893, -6.4856, -8.8243, -7.9221, -7.0221, -8.5078,\n",
            "         -6.3971]], grad_fn=<SqueezeBackward1>)\n",
            "end scores\n",
            " tensor([[-2.1800, -7.9309, -7.2973, -7.7770, -8.3925, -7.6919, -4.4972, -6.7296,\n",
            "         -6.0946, -6.5687, -7.1748, -2.1799, -5.8129, -3.8019, -4.9774, -4.3986,\n",
            "         -6.2847, -6.9633, -5.9137, -6.3668, -6.5247, -5.8524, -4.7968, -4.9602,\n",
            "         -4.9073, -4.6714, -7.3139, -7.7992, -3.0317, -3.3961, -7.3057, -6.9591,\n",
            "         -7.2449, -6.0153, -5.6170, -4.0029, -5.4750, -1.7832,  3.2250,  0.9834,\n",
            "         -0.3661,  7.6501,  7.0367,  3.9124, -4.7139, -5.7731, -6.3760, -6.6360,\n",
            "         -6.8261, -5.6096, -7.0054, -7.1603, -4.3861, -7.0941, -7.0665, -6.8154,\n",
            "         -3.2767, -6.8864, -6.6351, -5.6411, -2.2276, -3.9011, -6.9828, -6.9616,\n",
            "         -6.8569, -2.9957, -6.9848, -6.8305, -6.6821, -4.3279, -6.9769, -6.9032,\n",
            "         -3.5489, -6.9578, -4.9183, -3.4918, -3.5922, -7.0939, -5.7810, -7.0781,\n",
            "         -7.7444, -6.7201, -7.4529, -4.1731, -6.7722, -5.5946, -7.1350, -7.1302,\n",
            "         -4.6600, -4.2899, -7.3953, -7.2047, -4.9156, -5.6374, -4.0582, -4.7841,\n",
            "         -7.9910, -7.4010, -6.1832, -7.5263, -7.6231, -6.5592, -6.6838, -6.5272,\n",
            "         -4.9258, -7.5540, -7.2711, -4.6644, -7.3588, -6.6890, -4.1185, -4.8748,\n",
            "         -2.1801]], grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrUXw_t5z1i"
      },
      "source": [
        "\n",
        "\n",
        "Now we have start scores and end scores we can get both the start index and the end index and use both the indices for span prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLvn1GOI52qb",
        "outputId": "b31c8087-e7c0-47d9-b91d-1e69546c8fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "start_index = torch.argmax(start_scores)\n",
        "\n",
        "end_index = torch.argmax(end_scores)\n",
        "print(\"start index: \",start_index, \" End index: \", end_index)\n",
        "\n",
        "\n",
        "answer = ' '.join(tokens[start_index:end_index+1])\n",
        "print(answer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start index:  tensor(38)  End index:  tensor(41)\n",
            "weighted und ##ire ##cted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AESqGksv5_fz"
      },
      "source": [
        "\n",
        "\n",
        "Note: The model is likely to predict an end word that is before the start word. The correct way is to pick a span for which the total score (start score + end score) is maximum where end_index ≥ start_index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGYGD5fq6ZhS"
      },
      "source": [
        "Note: BERT uses wordpiece tokenization. Wordpiece split the tokens like “playing” to “play and ##ing”. It also covers a wider spectrum of Out-Of-Vocabulary (OOV) words.\n",
        "\n",
        "We can recover any words that were broken down into subwords with a little bit more work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GGNsPC46e9N"
      },
      "source": [
        "corrected_answer = ''\n",
        "\n",
        "for word in answer.split():\n",
        "    \n",
        "    #If it's a subword token\n",
        "    if word[0:2] == '##':\n",
        "        corrected_answer += word[2:]\n",
        "    else:\n",
        "        corrected_answer += ' ' + word"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbLBBuLa6f5r",
        "outputId": "8f5513b2-be2e-41a9-dcb8-1771c5f14fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(question)\n",
        "print(corrected_answer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prims algorithm works on what type of graph?\n",
            " weighted undirected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN527FRTX89p"
      },
      "source": [
        "## Questions asked on this text \n",
        "\n",
        "###question = '''prims algorithm works on what type of graph?'''\n",
        "###question = ''' how does Jarnik algorithm build a tree? '''\n",
        "###question = ''' how does Jarnik algorithm choose the next node of  a tree? '''\n",
        "###question = ''' What is a spanning tree? '''\n",
        "###question = ''' In a graph, if two nodes available how does Prims algorithm choose the next node? '''\n",
        "###question = ''' What is the other name for Prim's algorithm'''\n",
        "###question = ''' What category of algorithm does Prim fall into? '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paqy6mguTxdD"
      },
      "source": [
        "### Answers Given by the model\n",
        "- prims algorithm works on what type of graph?   \n",
        " weighted undirected\n",
        "\n",
        "- how does Jarnik algorithm build a tree?   \n",
        " one vertex at a time , from an arbitrary starting vertex\n",
        "- how does Jarnik algorithm choose the next node of  a tree? \n",
        " adding the cheapest possible connection from the tree to another vertex\n",
        "\n",
        "- What is a spanning tree? \n",
        " a subset of the edges that forms a tree that includes every vertex\n",
        "- In a graph, if two nodes available how does Prims algorithm coose the next node? \n",
        " adding the cheapest possible connection\n",
        "- What is the other name for Prim's algorithm     \n",
        "   jarnik ' s\n",
        "\n",
        "- What category of algorithm does Prim fall into? \n",
        "\n",
        " greedy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO5JtZJtbzif"
      },
      "source": [
        "## Questions not answered correctly by the model\n",
        "\n",
        "- Is prims and Jarniks same? \n",
        "\n",
        " prim ' s  ( Answer returned)\n",
        "\n",
        " It does not have a choice to say 'yes'. The answers should be available in the given text\n"
      ]
    }
  ]
}